{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 12, 1000)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load(\"/home/jkambulo/projects/def-rgmelko/jkambulo/Quantum-Computer-Temperature/data/KZ_Data/12x12/KZ_data_12x12_15_MHz_per_us.npz\")\n",
    "data['rydberg_data'][:,:,-1].shape\n",
    "# print(data['sweep_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 58\u001b[0m\n\u001b[1;32m     54\u001b[0m     energy \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(torch\u001b[39m.\u001b[39mexp(logp) \u001b[39m*\u001b[39m (detuning \u001b[39m+\u001b[39m interaction)) \u001b[39m+\u001b[39m rabi_energy\n\u001b[1;32m     56\u001b[0m     \u001b[39mreturn\u001b[39;00m energy\u001b[39m.\u001b[39mitem()\n\u001b[0;32m---> 58\u001b[0m compute_energy(torch\u001b[39m.\u001b[39;49mrandint(\u001b[39m0\u001b[39;49m,\u001b[39m2\u001b[39;49m,(\u001b[39m16\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m16\u001b[39;49m,)),np\u001b[39m.\u001b[39;49mlog(np\u001b[39m.\u001b[39;49mones((\u001b[39m16\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m16\u001b[39;49m,))\u001b[39m/\u001b[39;49m\u001b[39m256\u001b[39;49m), {\u001b[39m'\u001b[39;49m\u001b[39momega\u001b[39;49m\u001b[39m'\u001b[39;49m:\u001b[39m1\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mdelta_per_omega\u001b[39;49m\u001b[39m'\u001b[39;49m:\u001b[39m1\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrb_per_a\u001b[39;49m\u001b[39m'\u001b[39;49m:\u001b[39m1\u001b[39;49m},\u001b[39m256\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[3], line 51\u001b[0m, in \u001b[0;36mcompute_energy\u001b[0;34m(samples, logp, params, atoms)\u001b[0m\n\u001b[1;32m     48\u001b[0m     i \u001b[39m=\u001b[39m x \u001b[39m^\u001b[39m (\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39marange(atoms)[\u001b[39mNone\u001b[39;00m])  \u001b[39m# (2**16, 16)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39msum(torch\u001b[39m.\u001b[39mexp(\u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m (logp[i] \u001b[39m+\u001b[39m logp[:, \u001b[39mNone\u001b[39;00m])))\n\u001b[0;32m---> 51\u001b[0m detuning \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mdelta \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39;49msum(samples, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     52\u001b[0m interaction \u001b[39m=\u001b[39m C \u001b[39m*\u001b[39m compute_interaction(samples) \u001b[39m/\u001b[39m a\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m6\u001b[39m\n\u001b[1;32m     53\u001b[0m rabi_energy \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m \u001b[39mabs\u001b[39m(omega) \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m compute_rabi(logp) \u001b[39m# - omega to make wavefunction real\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def compute_energy(samples: torch.Tensor, logp: torch.Tensor, params: dict, atoms: int = 16):\n",
    "    \"\"\"Computes the exact energy based on all the probabilities of each state.\n",
    "\n",
    "    Args:\n",
    "        total_configs (torch.Tensor): tensor of all possible states of atoms. Should have shape (2**atoms, atoms)\n",
    "        logp (torch.Tensor): natural log of the probability of a state, shape (2**atoms,)\n",
    "        params (dict): Dictionary containing Hamiltonian tuning parameters {'omega', 'delta_per_omega', 'rb_per_a'}\n",
    "        atoms (int, optional): Number of atoms in grid, assumed to be a perfect square. Defaults to 16.\n",
    "\n",
    "    Returns:\n",
    "        float: Calculated energy expectation value\n",
    "    \"\"\"\n",
    "    required_params = {'omega', 'delta_per_omega', 'rb_per_a'}\n",
    "    assert len(set(params.keys()) ^ required_params) == 0, set(params.keys()) ^ required_params\n",
    "\n",
    "    C = 862690 * 2 * np.pi\n",
    "    omega = params['omega']\n",
    "    rb = (C / omega) ** (1 / 6)\n",
    "    a = rb / params['rb_per_a']\n",
    "    delta = params['delta_per_omega'] * omega\n",
    "    # print(f'c={C} omega={omega} rb={rb} a={a} delta={delta}')\n",
    "\n",
    "    def compute_interaction(x: torch.Tensor):\n",
    "        \"\"\"Computes sum_ij Vij*ni*nj\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): tensor of shape (batchsize, natoms)\n",
    "\n",
    "        Returns:\n",
    "            torch.tensor: tensor of shape (batchsize,)\n",
    "        \"\"\"\n",
    "        x = x.T\n",
    "        c = torch.arange(atoms)\n",
    "        rows = round(np.sqrt(atoms))\n",
    "        d_inv_6 = torch.triu((1/((c[None]%rows - c[:,None]%rows)**2 +\n",
    "                                (torch.div(c[None],rows, rounding_mode='floor') - torch.div(c[:,None],rows, rounding_mode='floor'))**2)**3),\n",
    "                            diagonal=1)\n",
    "        i, j = torch.triu_indices(atoms, atoms, offset=1)\n",
    "        filter = (x[i] == 1) & (x[j] == 1)\n",
    "        return torch.sum(d_inv_6[i[:, None] * filter, j[:, None] * filter], axis=0)\n",
    "\n",
    "    def compute_rabi(logp):\n",
    "        L = 2**atoms\n",
    "        x = torch.arange(L)[:, None]\n",
    "        i = x ^ (2 ** torch.arange(atoms)[None])  # (2**16, 16)\n",
    "        return torch.sum(torch.exp(0.5 * (logp[i] + logp[:, None])))\n",
    "\n",
    "    detuning = -delta * torch.sum(samples, axis=1)\n",
    "    interaction = C * compute_interaction(samples) / a**6\n",
    "    rabi_energy = - abs(omega) / 2 * compute_rabi(logp) # - omega to make wavefunction real\n",
    "    energy = torch.sum(torch.exp(logp) * (detuning + interaction)) + rabi_energy\n",
    "\n",
    "    return energy.item()\n",
    "\n",
    "compute_energy(torch.randint(0,2,(16*16,)),np.log(np.ones((16*16,))/256), {'omega':1, 'delta_per_omega':1, 'rb_per_a':1},256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "from contextlib import contextmanager\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def catchtime(text) -> float:\n",
    "    start = perf_counter()\n",
    "    yield lambda: perf_counter() - start\n",
    "    print(f'Time: {text} : {perf_counter() - start:.3f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jkambulo/projects/def-rgmelko/jkambulo/py10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RetNet(\n",
       "  (retnet): RetNetDecoder(\n",
       "    (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "    (embed_tokens): Embedding(2, 12)\n",
       "    (output_projection): Linear(in_features=12, out_features=2, bias=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-4): 5 x DecoderLayer(\n",
       "        (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "        (retention): MultiScaleRetention(\n",
       "          (q_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "          (k_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "          (v_proj): Linear(in_features=12, out_features=24, bias=True)\n",
       "          (g_proj): Linear(in_features=12, out_features=24, bias=True)\n",
       "          (out_proj): Linear(in_features=24, out_features=12, bias=True)\n",
       "          (group_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=False)\n",
       "        )\n",
       "        (retention_layer_norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): FeedForwardNetwork(\n",
       "          (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (fc1): Linear(in_features=12, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=12, bias=True)\n",
       "          (ffn_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "    (retnet_rel_pos): RetNetRelPos()\n",
       "  )\n",
       "  (log_softmax): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchscale.architecture.config import RetNetConfig\n",
    "from torchscale.architecture.retnet import RetNetDecoder\n",
    "\n",
    "# vocab size is before embedding. decoder_embed_dim is the number of dimensions in your embedding, ideally less than\n",
    "# config = RetNetConfig(vocab_size=2, decoder_embed_dim=20, decoder_ffn_embed_dim=1024, decoder_layers=3, decoder_retention_heads=5)\n",
    "# embedding = nn.Embedding(2, 20)\n",
    "# linear = nn.Linear(20,2)\n",
    "# retnet = RetNetDecoder(config, embed_tokens=embedding)\n",
    "# sum(param.numel() for param in retnet.parameters())\n",
    "from model_testing.models import RetNet\n",
    "retnet = RetNet(12,12)\n",
    "print(sum(p.numel() for p in retnet.parameters()))\n",
    "retnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 1, 2])\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     output, _ \u001b[39m=\u001b[39m retnet(x[:, i:i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m], incremental_state\u001b[39m=\u001b[39mincremental_state)\n\u001b[1;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(output\u001b[39m.\u001b[39mshape)\n\u001b[0;32m----> 6\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m()\n\u001b[1;32m      7\u001b[0m     \u001b[39m# output = retnet.linear(output)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[39m# output = retnet.log_softmax(x)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[39m# print(incremental_state)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(output\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mException\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x = torch.randint(0,2,(30,256))\n",
    "incremental_state = {}\n",
    "for i in range(256):\n",
    "    output, _ = retnet(x[:, i:i+1], incremental_state=incremental_state)\n",
    "    print(output.shape)\n",
    "    raise Exception()\n",
    "    # output = retnet.linear(output)\n",
    "    # output = retnet.log_softmax(x)\n",
    "    # print(incremental_state)\n",
    "print(output.shape)\n",
    "print(output)\n",
    "print(torch.argmax(output, dim=-1))\n",
    "print(output)\n",
    "print(incremental_state)\n",
    "with catchtime(\"parallel\"):\n",
    "    output, _ = retnet.forward(torch.randint(0,2,(200,256)))\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"occs\": shape (5,), type \"<i8\">\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "# with h5py.File('file.h5', 'w') as f:\n",
    "#     f['occs'] = np.array([1,2,3,4,5])\n",
    "    \n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def fileloader(filename):\n",
    "    try:\n",
    "        if filename.endswith('.jld') or filename.endswith(\".h5\"):\n",
    "            file = h5py.File(filename, 'r')\n",
    "        yield file\n",
    "    finally:\n",
    "        file.close()  \n",
    "    \n",
    "\n",
    "    \n",
    "with fileloader(\"file.h5\") as f:\n",
    "    print(f['occs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.566020131111145\n",
      "0.13552944362163544\n",
      "0.13326728343963623\n",
      "0.12589557468891144\n",
      "0.12600383162498474\n",
      "0.12744538486003876\n",
      "0.12763060629367828\n",
      "0.12687455117702484\n",
      "0.12541240453720093\n",
      "0.12825848162174225\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39m# print(output)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m# raise Exception('hey')\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39m# loss = loss_fun(output[order].view(-1, 2), target[order].view(-1))\u001b[39;00m\n\u001b[1;32m     29\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m torch\u001b[39m.\u001b[39mmean(F\u001b[39m.\u001b[39mone_hot(\u001b[39minput\u001b[39m, num_classes\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m*\u001b[39moutput)\n\u001b[0;32m---> 30\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     31\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     32\u001b[0m \u001b[39mprint\u001b[39m(loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/projects/def-rgmelko/jkambulo/py10/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/projects/def-rgmelko/jkambulo/py10/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "optimizer = torch.optim.Adam(retnet.parameters(), lr=0.0005, betas=(0.9, 0.98), eps=1e-9)\n",
    "data = np.load(\"/home/jkambulo/projects/def-rgmelko/jkambulo/Quantum-Computer-Temperature/data/KZ_Data/12x12/KZ_data_12x12_15_MHz_per_us.npz\")['rydberg_data']\n",
    "data = torch.from_numpy(data[:,:,1].reshape(12*12, -1)).type(torch.int64)\n",
    "\n",
    "# input = torch.tensor([[0,1,1,0,0,0],\n",
    "#                       [0,0,0,1,1,0],\n",
    "#                       [0,0,1,1,0,0],\n",
    "#                       [0,0,0,0,1,1],\n",
    "#                       [1,1,0,0,0,0]])\n",
    "# target = torch.tensor([[0,0,0,1,0,0],\n",
    "#                        [0,0,0,0,0,1],\n",
    "#                        [0,0,0,0,1,0],\n",
    "#                        [0,0,0,0,0,0],\n",
    "#                        [0,0,1,0,0,0]])\n",
    "# print(embedding(target).shape)\n",
    "# loss_fun = torch.nn.NLLLoss()\n",
    "batchsize = 100\n",
    "for epoch in range(1,30):\n",
    "    for i in range(data.shape[-1]//batchsize):\n",
    "        input = data[:,i*batchsize:(i+1)*batchsize].T\n",
    "        order = torch.randperm(5)\n",
    "        optimizer.zero_grad()\n",
    "        output = retnet(input)\n",
    "        # print(output)\n",
    "        # raise Exception('hey')\n",
    "        # loss = loss_fun(output[order].view(-1, 2), target[order].view(-1))\n",
    "        loss = - torch.mean(F.one_hot(input, num_classes=2)*output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss.item())\n",
    "        del output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (11x2 and 12x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m retnet(torch\u001b[39m.\u001b[39;49mtensor([[\u001b[39m0\u001b[39;49m,\u001b[39m1\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m]]))\u001b[39m.\u001b[39mexp()\n",
      "File \u001b[0;32m~/projects/def-rgmelko/jkambulo/py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/project/6000317/jkambulo/Quantum-Computer-Temperature/code/sweeping-energy/model_testing/models.py:72\u001b[0m, in \u001b[0;36mRetNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((torch\u001b[39m.\u001b[39mzeros(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m), x[:, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mint64)\n\u001b[1;32m     71\u001b[0m x, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretnet(x)\n\u001b[0;32m---> 72\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear(x)\n\u001b[1;32m     73\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_softmax(x)\n\u001b[1;32m     74\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/projects/def-rgmelko/jkambulo/py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/def-rgmelko/jkambulo/py10/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (11x2 and 12x2)"
     ]
    }
   ],
   "source": [
    "retnet(torch.tensor([[0,1,0,0,0,0,0,0,0,0,0]])).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jkambulo/projects/def-rgmelko/jkambulo/py10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "  File \"/tmp/ipykernel_45752/1939139578.py\", line 2, in <module>\n",
      "  File \"/project/6000317/jkambulo/Quantum-Computer-Temperature/code/sweeping-energy/model_testing/models.py\", line 85, in sample\n",
      "  File \"/project/6000317/jkambulo/Quantum-Computer-Temperature/code/sweeping-energy/torchscale/torchscale/architecture/retnet.py\", line 378, in forward\n",
      "  File \"/home/jkambulo/projects/def-rgmelko/jkambulo/py10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "  File \"/project/6000317/jkambulo/Quantum-Computer-Temperature/code/sweeping-energy/torchscale/torchscale/architecture/retnet.py\", line 184, in forward\n",
      "  File \"/home/jkambulo/projects/def-rgmelko/jkambulo/py10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "  File \"/project/6000317/jkambulo/Quantum-Computer-Temperature/code/sweeping-energy/torchscale/torchscale/component/feedforward_network.py\", line 132, in forward\n",
      "  File \"/home/jkambulo/projects/def-rgmelko/jkambulo/py10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "  File \"/home/jkambulo/projects/def-rgmelko/jkambulo/py10/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "RuntimeError: [enforce fail at alloc_cpu.cpp:75] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 2048000 bytes. Error code 12 (Cannot allocate memory)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jkambulo/projects/def-rgmelko/jkambulo/py10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2105, in showtraceback\n",
      "  File \"/home/jkambulo/projects/def-rgmelko/jkambulo/py10/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1428, in structured_traceback\n",
      "  File \"/home/jkambulo/projects/def-rgmelko/jkambulo/py10/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1319, in structured_traceback\n",
      "  File \"/home/jkambulo/projects/def-rgmelko/jkambulo/py10/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1172, in structured_traceback\n",
      "  File \"/home/jkambulo/projects/def-rgmelko/jkambulo/py10/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1062, in format_exception_as_a_whole\n",
      "  File \"/home/jkambulo/projects/def-rgmelko/jkambulo/py10/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1137, in get_records\n",
      "  File \"/home/jkambulo/projects/def-rgmelko/jkambulo/py10/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 184, in get_line_number_of_frame\n",
      "  File \"/home/jkambulo/projects/def-rgmelko/jkambulo/py10/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 149, in count_lines_in_py_file\n",
      "MemoryError\n"
     ]
    }
   ],
   "source": [
    "with catchtime('time'):\n",
    "    retnet.sample(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retnet.forward(torch.tensor([1,0,0,0,1,1,0,1,0,1]))\n",
    "1216/19"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
